{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "dcc6d272",
      "metadata": {
        "id": "dcc6d272",
        "outputId": "0c67dd12-ca1e-439f-f37a-d3b149124601"
      },
      "outputs": [],
      "source": [
        "# https://github.com/DiveshRKubal/GenerativeAI/blob/main/Variational%20Autoencoders/Variational_Autoencoders_Implementation.ipynb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import (\n",
        "    layers,\n",
        "    models,\n",
        "    datasets,\n",
        "    callbacks,\n",
        "    losses,\n",
        "    optimizers,\n",
        "    metrics,\n",
        ")\n",
        "\n",
        "import keras\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras_tuner\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, Lambda\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "# disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "eeaf0356",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the desired number of samples for each class\n",
        "class_samples = [2030, 3020]  # Adjust these numbers as needed\n",
        "\n",
        "# Calculate weights based on the desired number of samples\n",
        "class_weights = [num_samples / sum(class_samples) for num_samples in class_samples]\n",
        "\n",
        "\n",
        "# Generate a synthetic dataset with different numbers of samples for each class\n",
        "X, y = make_classification(\n",
        "    n_samples=sum(class_samples),\n",
        "    n_features=4,\n",
        "    n_informative=4,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    weights=class_weights,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "\n",
        "# Create a Pandas DataFrame with the generated data\n",
        "columns = [\"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\"]\n",
        "synthetic_df = pd.DataFrame(data=X, columns=columns)\n",
        "\n",
        "for column in synthetic_df:\n",
        "    std = np.std(synthetic_df[column])\n",
        "    mean = np.mean(synthetic_df[column])\n",
        "    synthetic_df[column] = synthetic_df[column]-mean\n",
        "    synthetic_df[column] = synthetic_df[column]/std\n",
        "\n",
        "\n",
        "synthetic_df[\"target\"] = y\n",
        "\n",
        "# Display the first few rows of the synthetic dataset\n",
        "synthetic_array =synthetic_df.values\n",
        "train_data, test_data = train_test_split(\n",
        "    synthetic_array, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "2a190736",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "372355fd",
      "metadata": {
        "id": "372355fd"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc8330f",
      "metadata": {
        "id": "ecc8330f"
      },
      "source": [
        "# Designing and Building Variational Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ooxVTwrujYh5",
      "metadata": {
        "id": "ooxVTwrujYh5"
      },
      "source": [
        "First, we need to create a new Sampling layer for sampling from the distribution defined by z_mean and z_log_var."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e860bbf5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of combinations: 2160\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "# https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\n",
        "\n",
        "list_of_layers = [8, 32, 128, 256, 512, 1024]\n",
        "list_of_latent = [2, 4, 16, 128, 256]\n",
        "list_of_boolean = [True, False]\n",
        "list_of_drop_out = [0.1, 0.3, 0.5]\n",
        "\n",
        "\n",
        "hyperparameters_product = product(\n",
        "    list_of_layers, list_of_layers, list_of_latent, list_of_boolean, list_of_boolean,\n",
        "    list_of_drop_out\n",
        ")\n",
        "# Calculate the total number of combinations\n",
        "number_of_combinations = len(list(hyperparameters_product))\n",
        "\n",
        "print(\"Total number of combinations:\", number_of_combinations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "05464323",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "HP_NUM_embedded = hp.HParam(\"num_embedded\", hp.Discrete(list_of_latent))\n",
        "HP_NUM_UNITS = hp.HParam(\"num_units\", hp.Discrete(list_of_layers))\n",
        "HP_NUM_UNITS1 = hp.HParam(\"num_units1\", hp.Discrete(list_of_layers))\n",
        "HP_BOOL_BATCH = hp.HParam(\"bool_batch\", hp.Discrete(list_of_boolean))\n",
        "HP_DROP_OUT_BOOL = hp.HParam(\"drop_out\", hp.Discrete(list_of_boolean))\n",
        "HP_DROP_OUT_RATE = hp.HParam(\"drop_out_rate\", hp.Discrete(list_of_drop_out))\n",
        "VAL_LOSS = \"val_loss\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with tf.summary.create_file_writer(\"logs/hparam_tuning_new2/\").as_default():\n",
        "    hp.hparams_config(\n",
        "        hparams=[\n",
        "            HP_NUM_embedded,\n",
        "            HP_BOOL_BATCH,\n",
        "            HP_NUM_UNITS,\n",
        "            HP_NUM_UNITS1,\n",
        "            HP_DROP_OUT_BOOL,\n",
        "            HP_DROP_OUT_RATE,\n",
        "        ],\n",
        "        metrics=[hp.Metric(VAL_LOSS, display_name=\"val_loss\")],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a6d2dbf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(hparams):\n",
        "    class Sampling(layers.Layer):\n",
        "        # We create a new layer by subclassing the keras base Layer\n",
        "        def call(self, inputs):\n",
        "            z_mean, z_log_var = inputs\n",
        "            batch = tf.shape(z_mean)[0]\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon = K.random_normal(shape=(batch, dim))\n",
        "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    embed_dim = hparams[HP_NUM_embedded]\n",
        "\n",
        "    encoder_input = layers.Input(shape=(5,), name=\"encoder_input\")\n",
        "    x = layers.Dense(hparams[HP_NUM_UNITS], activation=\"relu\", name=\"h1\")(encoder_input)\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.Dropout(hparams[HP_DROP_OUT_RATE])(x)\n",
        "\n",
        "    x = layers.Dense(hparams[HP_NUM_UNITS1], activation=\"relu\", name=\"h2\")(x)\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    n_x3 = hparams[HP_NUM_UNITS1]\n",
        "\n",
        "    x = layers.Dense(n_x3, activation=\"relu\", name=\"h3\")(x)\n",
        "\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.Dropout(hparams[HP_DROP_OUT_RATE])(x)\n",
        "\n",
        "    # Split x3 into two halves\n",
        "    half_size = n_x3 // 2\n",
        "    x3_first_half = layers.Lambda(lambda x: x[:, :half_size], name=\"select_z_mean\")(x)\n",
        "    x3_second_half = layers.Lambda(lambda x: x[:, half_size:], name=\"select_z_var\")(x)\n",
        "\n",
        "    z_mean = layers.Dense(embed_dim, name=\"z_mean\")(x3_first_half)\n",
        "    z_log_var = layers.Dense(embed_dim, name=\"z_log_var\")(x3_second_half)\n",
        "\n",
        "    # The Sampling layer samples a point z in the latent space from the\n",
        "    # normal distribution defined by the parameters z_mean and z_log_var.\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "    # The Keras Model that defines the encoderâ€”a model that takes an input\n",
        "    # image and outputs z_mean, z_log_var and a sampled point z from the\n",
        "    # normal distribution defined by these parameters.\n",
        "    encoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    # Decoder\n",
        "    decoder_input = layers.Input(shape=(embed_dim,), name=\"decoder_input\")\n",
        "\n",
        "    x = layers.Dense(hparams[HP_NUM_UNITS1], activation=\"relu\", name=\"h4\")(\n",
        "        decoder_input\n",
        "    )\n",
        "\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.Dropout(hparams[HP_DROP_OUT_RATE])(x)\n",
        "\n",
        "    x = layers.Dense(hparams[HP_NUM_UNITS1], activation=\"relu\", name=\"h5\")(x)\n",
        "\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.Dropout(hparams[HP_DROP_OUT_RATE])(x)\n",
        "\n",
        "    n_x6 = hparams[HP_NUM_UNITS]\n",
        "\n",
        "    x = layers.Dense(n_x6, activation=\"relu\", name=\"h6\")(x)\n",
        "\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if hparams[HP_BOOL_BATCH] == True:\n",
        "        x = layers.Dropout(hparams[HP_DROP_OUT_RATE])(x)\n",
        "\n",
        "    # Split x6 into two parts (80% and 20%)\n",
        "    cont_decoder_input = layers.Lambda(\n",
        "        lambda x: x[:, : int(4 / 5 * n_x6)], name=\"select_x6_cont\"\n",
        "    )(x)\n",
        "    class_decoder_input = layers.Lambda(\n",
        "        lambda x: x[:, int(4 / 5 * n_x6) :], name=\"select_x6_class\"\n",
        "    )(x)\n",
        "\n",
        "    cont_decoder_outputs = layers.Dense(\n",
        "        4, activation=\"linear\", name=\"cont_decoder_output\"\n",
        "    )(cont_decoder_input)\n",
        "    class_decoder_output = layers.Dense(\n",
        "        1, activation=\"sigmoid\", name=\"classification_output\"\n",
        "    )(class_decoder_input)\n",
        "\n",
        "    decoder = models.Model(decoder_input, [cont_decoder_outputs, class_decoder_output])\n",
        "\n",
        "    class VAE(models.Model):\n",
        "        def __init__(self, encoder, decoder, **kwargs):\n",
        "            super(VAE, self).__init__(**kwargs)\n",
        "            self.encoder = encoder\n",
        "            self.decoder = decoder\n",
        "            self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n",
        "            self.reconstruction_loss_tracker_cont = metrics.Mean(\n",
        "                name=\"reconstruction_loss_cont\"\n",
        "            )\n",
        "            self.reconstruction_loss_tracker_class = metrics.Mean(\n",
        "                name=\"reconstruction_loss_class\"\n",
        "            )\n",
        "\n",
        "            self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "        @property\n",
        "        def metrics(self):\n",
        "            return [\n",
        "                self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker_cont,\n",
        "                self.reconstruction_loss_tracker_class,\n",
        "                self.kl_loss_tracker,\n",
        "            ]\n",
        "\n",
        "        def call(self, inputs):\n",
        "            \"\"\"Call the model on a particular input.\"\"\"\n",
        "            z_mean, z_log_var, z = encoder(inputs)\n",
        "            reconstruction = decoder(z)\n",
        "            return z_mean, z_log_var, reconstruction\n",
        "\n",
        "        def train_step(self, data):\n",
        "            \"\"\"Step run during training.\"\"\"\n",
        "            with tf.GradientTape() as tape:\n",
        "                # TensorFlow's Gradient Tape helps calculate gradients during a forward pass.\n",
        "                # To use it, we need wrap the code that performs the operations you want to differentiate within a tf.GradientTape() context.\n",
        "                # After recording the operations, we can compute the gradient of the loss function concerning certain variables using tape.gradient().\n",
        "                # These gradients are then used to update the variables with the optimizer.\n",
        "\n",
        "                z_mean, z_log_var, reconstruction = self(data)\n",
        "                beta = 500\n",
        "                reconstruction_loss_cont = tf.reduce_mean(\n",
        "                    beta * losses.mean_squared_error(data[:, :4], reconstruction[0])\n",
        "                )\n",
        "                reconstruction_loss_class = tf.reduce_mean(\n",
        "                    beta * losses.binary_crossentropy(data[:, 4:], reconstruction[1])\n",
        "                )\n",
        "                kl_loss = tf.reduce_mean(\n",
        "                    tf.reduce_sum(\n",
        "                        -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n",
        "                        axis=1,\n",
        "                    )\n",
        "                )\n",
        "                total_loss = (\n",
        "                    reconstruction_loss_cont + reconstruction_loss_class + kl_loss\n",
        "                )\n",
        "\n",
        "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "            self.total_loss_tracker.update_state(total_loss)\n",
        "            self.reconstruction_loss_tracker_cont.update_state(reconstruction_loss_cont)\n",
        "            self.reconstruction_loss_tracker_class.update_state(\n",
        "                reconstruction_loss_class\n",
        "            )\n",
        "            self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "            return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "        def test_step(self, data):\n",
        "            \"\"\"Step run during validation.\"\"\"\n",
        "            if isinstance(data, tuple):\n",
        "                data = data[0]\n",
        "\n",
        "            z_mean, z_log_var, reconstruction = self(data)\n",
        "            beta = 500\n",
        "            reconstruction_loss_cont = tf.reduce_mean(\n",
        "                beta * losses.mean_squared_error(data[:, :4], reconstruction[0])\n",
        "            )\n",
        "            reconstruction_loss_class = tf.reduce_mean(\n",
        "                beta * losses.binary_crossentropy(data[:, 4:], reconstruction[1])\n",
        "            )\n",
        "            kl_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n",
        "                    axis=1,\n",
        "                )\n",
        "            )\n",
        "            total_loss = reconstruction_loss_cont + reconstruction_loss_class + kl_loss\n",
        "\n",
        "            mean_diff = tf.reduce_mean(data[:, :4] - reconstruction[0])\n",
        "\n",
        "            return {\n",
        "                \"loss\": total_loss,\n",
        "                \"reconstruction_loss_cont\": reconstruction_loss_cont,\n",
        "                \"reconstruction_loss_class\": reconstruction_loss_class,\n",
        "                \"kl_loss\": kl_loss,\n",
        "                \"mean_diff\": mean_diff,\n",
        "            }\n",
        "\n",
        "    log_dir = \"logs/\"\n",
        "    vae = VAE(encoder, decoder)\n",
        "    optimizer = optimizers.Adam(learning_rate=0.0005)\n",
        "    vae.compile(optimizer=optimizer)\n",
        "    hist = vae.fit(\n",
        "        train_data,\n",
        "        epochs=1,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        validation_data=(test_data, test_data),\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.TensorBoard(log_dir),  # log metrics\n",
        "            hp.KerasCallback(log_dir, hparams),  # log hparams\n",
        "        ],\n",
        "    )\n",
        "    return hist.history[\"val_loss\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "81e0d97f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run(run_dir, hparams):\n",
        "    with tf.summary.create_file_writer(run_dir).as_default():\n",
        "        hp.hparams(hparams)  # record the values used in this trial\n",
        "        val_loss = build_model(hparams)[0]\n",
        "        tf.summary.scalar(VAL_LOSS, val_loss, step=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "8d839aed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Next run number: 1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the logs directory\n",
        "logs_dir = \"logs/hparam_tuning_new2/\"\n",
        "\n",
        "# Check if the logs directory exists\n",
        "if os.path.exists(logs_dir):\n",
        "    # Get a list of all files in the logs directory\n",
        "    log_files = os.listdir(logs_dir)\n",
        "\n",
        "    # Filter the list to only include files with the naming convention \"run-<number>\"\n",
        "    run_numbers = [\n",
        "        int(file.split(\"-\")[1]) for file in log_files if file.startswith(\"run-\")\n",
        "    ]\n",
        "\n",
        "    # Determine the next run number by finding the maximum run number and incrementing it by 1\n",
        "    next_run_num = max(run_numbers) + 1 if run_numbers else 0\n",
        "else:\n",
        "    # If the logs directory doesn't exist, start from run-0\n",
        "    next_run_num = 0\n",
        "\n",
        "print(\"Next run number:\", next_run_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "532fb783",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting trial: run-1\n",
            "{'num_embedded': 2, 'num_units': 8, 'num_units1': 8, 'bool_batch': False, 'drop_out_rate': 0}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "253/253 [==============================] - 3s 3ms/step - total_loss: 840.8922 - reconstruction_loss_cont: 497.7945 - reconstruction_loss_class: 342.9664 - kl_loss: 0.1309 - val_loss: 677.0210 - val_reconstruction_loss_cont: 291.3513 - val_reconstruction_loss_class: 385.6054 - val_kl_loss: 0.0643 - val_mean_diff: -0.6859\n",
            "--- Starting trial: run-2\n",
            "{'num_embedded': 2, 'bool_batch': True, 'num_units': 8, 'num_units1': 8, 'drop_out_rate': 0.1}\n",
            "253/253 [==============================] - 3s 5ms/step - total_loss: 830.0598 - reconstruction_loss_cont: 487.5257 - reconstruction_loss_class: 342.0951 - kl_loss: 0.4393 - val_loss: 656.2625 - val_reconstruction_loss_cont: 254.6931 - val_reconstruction_loss_class: 401.4139 - val_kl_loss: 0.1554 - val_mean_diff: -0.6151\n",
            "--- Starting trial: run-3\n",
            "{'num_embedded': 2, 'bool_batch': True, 'num_units': 8, 'num_units1': 8, 'drop_out_rate': 0.3}\n",
            "253/253 [==============================] - 3s 4ms/step - total_loss: 841.2646 - reconstruction_loss_cont: 500.9435 - reconstruction_loss_class: 339.8170 - kl_loss: 0.5037 - val_loss: 739.3125 - val_reconstruction_loss_cont: 298.7776 - val_reconstruction_loss_class: 440.1086 - val_kl_loss: 0.4263 - val_mean_diff: -0.6864\n",
            "--- Starting trial: run-4\n",
            "{'num_embedded': 2, 'bool_batch': True, 'num_units': 8, 'num_units1': 8, 'drop_out_rate': 0.5}\n",
            "253/253 [==============================] - 4s 4ms/step - total_loss: 841.0838 - reconstruction_loss_cont: 495.7622 - reconstruction_loss_class: 345.2943 - kl_loss: 0.0277 - val_loss: 667.9180 - val_reconstruction_loss_cont: 276.3194 - val_reconstruction_loss_class: 391.5732 - val_kl_loss: 0.0254 - val_mean_diff: -0.6627\n",
            "--- Starting trial: run-5\n",
            "{'num_embedded': 2, 'num_units': 8, 'num_units1': 32, 'bool_batch': False, 'drop_out_rate': 0}\n",
            "253/253 [==============================] - 3s 4ms/step - total_loss: 813.2144 - reconstruction_loss_cont: 474.8532 - reconstruction_loss_class: 337.7896 - kl_loss: 0.5714 - val_loss: 674.8157 - val_reconstruction_loss_cont: 288.9434 - val_reconstruction_loss_class: 385.8126 - val_kl_loss: 0.0597 - val_mean_diff: -0.6827\n",
            "--- Starting trial: run-6\n",
            "{'num_embedded': 2, 'bool_batch': True, 'num_units': 8, 'num_units1': 32, 'drop_out_rate': 0.1}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Starting trial: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m run_name)\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m({h\u001b[38;5;241m.\u001b[39mname: hparams[h] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hparams})\n\u001b[1;32m---> 19\u001b[0m         \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogs/hparam_tuning_new2/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m         session_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "Cell \u001b[1;32mIn[31], line 4\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_dir, hparams)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mcreate_file_writer(run_dir)\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m      3\u001b[0m     hp\u001b[38;5;241m.\u001b[39mhparams(hparams)  \u001b[38;5;66;03m# record the values used in this trial\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m     tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mscalar(VAL_LOSS, val_loss, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[1;32mIn[30], line 203\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(hparams)\u001b[0m\n\u001b[0;32m    201\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[0;32m    202\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m--> 203\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorBoard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# log metrics\u001b[39;49;00m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKerasCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# log hparams\u001b[39;49;00m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hist\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:963\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    962\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 963\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    966\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:785\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_stateful_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_get_concrete_function_internal_garbage_collected(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    786\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    789\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2523\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2522\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 2523\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2760\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2758\u001b[0m   \u001b[38;5;66;03m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[0;32m   2759\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m placeholder_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m-> 2760\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2762\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_capture_func_lib  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m \u001b[38;5;66;03m# Maintain the list of all captures\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2670\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2666\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2667\u001b[0m ]\n\u001b[0;32m   2668\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   2669\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 2670\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2671\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2672\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2673\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2675\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2678\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   2680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   2681\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   2682\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   2684\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   2686\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2687\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1153\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m   deps_control_manager \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mNullContextmanager()\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default(), deps_control_manager \u001b[38;5;28;01mas\u001b[39;00m deps_ctx:\n\u001b[0;32m   1154\u001b[0m   current_scope \u001b[38;5;241m=\u001b[39m variable_scope\u001b[38;5;241m.\u001b[39mget_variable_scope()\n\u001b[0;32m   1155\u001b[0m   default_use_resource \u001b[38;5;241m=\u001b[39m current_scope\u001b[38;5;241m.\u001b[39muse_resource\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py:446\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Ignore switches (they're handled separately)\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSwitch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m op\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes_module\u001b[38;5;241m.\u001b[39mresource:\n\u001b[0;32m    447\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Make merges trigger all other computation which must run\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Don't do this. Write a transform to chains instead.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# See core/common_runtime/control_flow_deps_to_chains.cc.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Max_G\\anaconda3\\envs\\var_auto\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2583\u001b[0m, in \u001b[0;36mOperation.type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   2581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   2582\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2583\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_OperationOpType\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_op\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "session_num = next_run_num\n",
        "\n",
        "for embedded in HP_NUM_embedded.domain.values:\n",
        "    for num_units in HP_NUM_UNITS.domain.values:\n",
        "        for num_units1 in HP_NUM_UNITS1.domain.values:\n",
        "            for drop_out_enc in HP_BOOL_BATCH.domain.values:\n",
        "                if drop_out_enc:  # Only execute if drop_out_enc is True\n",
        "                    for drop_out_rate in HP_DROP_OUT_RATE.domain.values:\n",
        "                        hparams = {\n",
        "                            HP_NUM_embedded: embedded,\n",
        "                            HP_BOOL_BATCH: drop_out_enc,\n",
        "                            HP_NUM_UNITS: num_units,\n",
        "                            HP_NUM_UNITS1: num_units1,\n",
        "                            HP_DROP_OUT_RATE: drop_out_rate,\n",
        "                        }\n",
        "                        run_name = \"run-%d\" % session_num\n",
        "                        print(\"--- Starting trial: %s\" % run_name)\n",
        "                        print({h.name: hparams[h] for h in hparams})\n",
        "                        run(\"logs/hparam_tuning_new2/\" + run_name, hparams)\n",
        "                        session_num += 1\n",
        "                else:\n",
        "                    hparams = {\n",
        "                        HP_NUM_embedded: embedded,\n",
        "                        HP_NUM_UNITS: num_units,\n",
        "                        HP_NUM_UNITS1: num_units1,\n",
        "                        HP_BOOL_BATCH: drop_out_enc,\n",
        "                        HP_DROP_OUT_RATE: 0,\n",
        "                    }\n",
        "                    run_name = \"run-%d\" % session_num\n",
        "                    print(\"--- Starting trial: %s\" % run_name)\n",
        "                    print({h.name: hparams[h] for h in hparams})\n",
        "                    run(\"logs/hparam_tuning_new2/\" + run_name, hparams)\n",
        "                    session_num += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
